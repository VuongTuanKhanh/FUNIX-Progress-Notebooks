{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"lab-5.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"cnD1ecF0Mw0u","colab_type":"text"},"source":["# Feature Selection and LASSO (Interpretation)"]},{"cell_type":"markdown","metadata":{"id":"N6oYlO_fMw0z","colab_type":"text"},"source":["In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO (in sklearn, obviously). You will:\n","* Run LASSO with different L1 penalties.\n","* Choose best L1 penalty using a validation set.\n","* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n","\n","In the next exercise, you will implement your own LASSO solver, using coordinate descent. "]},{"cell_type":"markdown","metadata":{"id":"wRSBur8HMw02","colab_type":"text"},"source":["## The usual"]},{"cell_type":"code","metadata":{"id":"MjaAEidBMw05","colab_type":"code","colab":{}},"source":["import sklearn\n","import pandas\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBmiSN8sMw1A","colab_type":"text"},"source":["## Load in house sales data\n","\n","Dataset is from house sales in King County, the region where the city of Seattle, WA is located. *I am so surprised.*"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7cDLiqAIMw1D","colab_type":"code","colab":{}},"source":["full_data = pandas.read_csv(\"kc_house_data.csv\", index_col=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyLeYGvlMw1J","colab_type":"text"},"source":["## Create new features"]},{"cell_type":"markdown","metadata":{"id":"XSMOdbHVMw1L","colab_type":"text"},"source":["As in Lab 2 (*lab-2.ipynb*), we consider features that are some transformations of inputs."]},{"cell_type":"code","metadata":{"id":"mYzffDTvMw1M","colab_type":"code","colab":{}},"source":["from math import log, sqrt\n","full_data['sqft_living_sqrt'] = full_data['sqft_living'].map(sqrt)\n","full_data['sqft_lot_sqrt'] = full_data['sqft_lot'].map(sqrt)\n","full_data['bedrooms_square'] = full_data['bedrooms'] ** 2\n","\n","# In the dataset, 'floors' was defined with type string, \n","# so we'll convert them to float, before creating a new feature.\n","full_data['floors'] = full_data['floors'].astype(float) \n","full_data['floors_square'] = full_data['floors'] ** 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z672nh0kMw1S","colab_type":"text"},"source":["* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n","* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."]},{"cell_type":"markdown","metadata":{"id":"VDo2w5chMw1U","colab_type":"text"},"source":["# Learn regression weights with L1 penalty"]},{"cell_type":"markdown","metadata":{"id":"u-xMl9bUMw1V","colab_type":"text"},"source":["Let us fit a model with all the features available, plus the features we just created above."]},{"cell_type":"code","metadata":{"id":"OYAUT1PLMw1W","colab_type":"code","colab":{}},"source":["all_features = ['bedrooms', 'bedrooms_square',\n","            'bathrooms',\n","            'sqft_living', 'sqft_living_sqrt',\n","            'sqft_lot', 'sqft_lot_sqrt',\n","            'floors', 'floors_square',\n","            'waterfront', 'view', 'condition', 'grade',\n","            'sqft_above',\n","            'sqft_basement',\n","            'yr_built', 'yr_renovated']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7-592ikMw1e","colab_type":"text"},"source":["Applying L1 penalty requires adding an extra parameter (`alpha=l1_penalty`) to the sklearn model `Lasso`. (Other tools may have separate implementations of LASSO). Much like L2/Ridge Regression, the features should be scaled to ensure equal attention inbetween."]},{"cell_type":"code","metadata":{"id":"KlTbDC0RMw1g","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import Lasso\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","l1_penalty=5e4\n","full_features = scaler.fit_transform(full_data[all_features].values)\n","full_labels = full_data['price'].values\n","model = Lasso(alpha=l1_penalty).fit(full_features, full_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rH4847CcMw1m","colab_type":"text"},"source":["Find what features had non-zero weight."]},{"cell_type":"code","metadata":{"id":"HElqPj8LMw1n","colab_type":"code","colab":{}},"source":["# Do you know that even numpy has built-in boolean selector?"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0gF5MD_Mw1r","colab_type":"text"},"source":["Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n","\n","***QUIZ QUESTION***:\n","According to this list of weights, which of the features have been chosen? "]},{"cell_type":"markdown","metadata":{"id":"9Zv3CFZBMw1s","colab_type":"text"},"source":["# Selecting an L1 penalty"]},{"cell_type":"markdown","metadata":{"id":"Oc6Vm3RfMw1t","colab_type":"text"},"source":["To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n","* Split our sales data into 2 sets: training and test (9/1)\n","* Further split our training data into two sets: train, validation (5/5)\n","\n","Be *very* careful that you use seed = 1 to ensure you get the same answer!"]},{"cell_type":"code","metadata":{"id":"U41_wuIcMw1v","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","# Cookie for those who instantly remember what cell to copy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7kCoG3VwMw10","colab_type":"text"},"source":["Next, we write a loop that does the following:\n","* For `l1_penalty` in 21 steps range between [1, 10^9] (use `np.logspace(0, 9, num=21)`.)\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n","    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n","* Report which `l1_penalty` produced the lowest RSS on validation data."]},{"cell_type":"code","metadata":{"id":"kYf8zDJtMw11","colab_type":"code","colab":{}},"source":["# Handholding level: Nominally.\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAneHq6LMw15","colab_type":"text"},"source":["*** QUIZ QUESTION. *** What was the best value for the `l1_penalty`?"]},{"cell_type":"code","metadata":{"id":"oZ1BXWL7Mw16","colab_type":"code","colab":{}},"source":["# Eyeball power, or computation power?"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PcnSvUHyMw19","colab_type":"text"},"source":["***QUIZ QUESTION***\n","Also, using this value of L1 penalty, how many nonzero weights do you have?"]},{"cell_type":"code","metadata":{"id":"Od-dOA-IMw1-","colab_type":"code","colab":{}},"source":["# Interesting, isn't it"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_zNo9ywLMw2F","colab_type":"text"},"source":["# Limit the number of nonzero weights\n","\n","What if we absolutely wanted to limit ourselves to, say, 5 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."]},{"cell_type":"markdown","metadata":{"id":"jLijk-3UMw2G","colab_type":"text"},"source":["In this section, you are going to implement a simple, two phase procedure to achive this goal:\n","1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n","2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."]},{"cell_type":"code","metadata":{"id":"kOwQHIzMMw2H","colab_type":"code","colab":{}},"source":["max_nonzeros = 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMv9UnluMw2N","colab_type":"text"},"source":["## Exploring the larger range of values to find a narrow range with the desired sparsity\n","\n","Let's define a wide range of possible `l1_penalty_values`:"]},{"cell_type":"code","metadata":{"id":"vEaNt8VHMw2O","colab_type":"code","colab":{}},"source":["l1_penalty_values = np.logspace(3, 5, num=21)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mxrj8371Mw2S","colab_type":"text"},"source":["Now, implement a loop that search through this space of possible `l1_penalty` values:\n","\n","* For `l1_penalty` in `np.logspace(3, 5, num=21)`:\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n","    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list.\n","        * *Hint: `model.coef_` gives you the coefficients/parameters you learned (barring intercept) in the form of numpy array. You can then use array\\[condition\\] for the list of values passing the condition. Or just use the builtin `np.count_nonzero()`"]},{"cell_type":"code","metadata":{"id":"_kFzVsaiMw2U","colab_type":"code","colab":{}},"source":["# Or you can do it with pure python. Its impact on the performance is negligible."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ucgWXpj9Mw2Z","colab_type":"text"},"source":["Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n","\n","More formally, find:\n","* The smallest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n","    * Store this value in the variable `l1_penalty_min` (we will use it later)\n","* The biggest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n","    * Store this value in the variable `l1_penalty_max` (we will use it later)\n","\n","\n","*Hint: there are many ways to do this, e.g.:*\n","* Programmatically within the loop above\n","* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."]},{"cell_type":"code","metadata":{"id":"UBpwiXEVMw2b","colab_type":"code","colab":{}},"source":["l1_penalty_min = #\n","l1_penalty_max = #"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-S2jtklaMw2g","colab_type":"text"},"source":["***QUIZ QUESTION.*** What values did you find for `l1_penalty_min` and `l1_penalty_max`, respectively? "]},{"cell_type":"markdown","metadata":{"id":"-T_xUbJ5Mw2i","colab_type":"text"},"source":["## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set \n","\n","We will now explore the narrow region of `l1_penalty` values we found:"]},{"cell_type":"code","metadata":{"id":"mEICQwrAMw2k","colab_type":"code","colab":{}},"source":["l1_penalty_values = np.linspace(l1_penalty_min,l1_penalty_max,20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVkWedu9Mw2p","colab_type":"text"},"source":["* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty`.\n","    * Measure the RSS of the learned model on the VALIDATION set\n","\n","Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`."]},{"cell_type":"code","metadata":{"id":"gKsCKTP3Mw2q","colab_type":"code","colab":{}},"source":["# See the quiz below as well"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4vAuKXfMw2u","colab_type":"text"},"source":["***QUIZ QUESTIONS***\n","1. What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?\n","2. What features in this model have non-zero coefficients?"]},{"cell_type":"code","metadata":{"id":"VISUoAMIMw2v","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}